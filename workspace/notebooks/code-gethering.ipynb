{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, re\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy import displacy\n",
    "import visualise_spacy_tree\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# load englis language model\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['ner','textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Path\n",
    "# folders = glob.glob('data/data-text')\n",
    "\n",
    "# Dataframe\n",
    "# df = pd.DataFrame(columns={'original','Sentence'})\n",
    "\n",
    "# Read in all text files into DF\n",
    "# df = pd.read_csv('../data/data-text/01 Antonet - Four-Canals-Carocci.txt',sep=\" \",header=None)\n",
    "# what is happening here is that each word is split into separate cell in df\n",
    "# this is not what I want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"An engineer had to plan the construction of an artificial lake to produce electric energy. To feed the lake he thought to build a unique wide canal collecting water coming from a near valley. However, a mason pointed out that during the flood periods the stream of water flowing along the canal might be too strong and might damage the surrounding areas; by contrast, during the drought periods a unique stream of water might be insufficient to feed the lake. In order to avoid these mishaps, the mason suggested to build, instead of a unique wide canal, four small canals whose total flow was the same as the unique wide canal previously planned. These small canals were placed around the lake so that they conveyed water coming from four different valleys. In this way only small amounts of water could flow in each canal and thus during flood periods dangerous overflowing might not occur. At the same time, the lake was fed by water from various belts, so that also during drought periods it was sufficiency that the fed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "type(doc[0].text) #str\n",
    "type(doc[0]) # spacy.tokens.token.Token\n",
    "word_in_text = [token.text for token in doc]\n",
    "coares_grained_POS = [token.pos_ for token in doc]\n",
    "fine_grained_POS = [token.tag_ for token in doc]\n",
    "# Syntactic dependencies (predicted by statistical model)\n",
    "# Dependency labels\n",
    "dep_labels = [token.dep_ for token in doc]\n",
    "# Syntactic head token (governor)\n",
    "governor = [token.head.text for token in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "It          PRON      nsubj     \n’s          VERB      ccomp     \nofficial    ADJ       dobj      \n:           PUNCT     punct     \nApple       PROPN     nsubj     \nis          AUX       ROOT      \nthe         DET       det       \nfirst       ADJ       amod      \nU.S.        PROPN     nmod      \npublic      ADJ       amod      \ncompany     NOUN      attr      \nto          PART      aux       \nreach       VERB      relcl     \na           DET       det       \n$           SYM       quantmod  \n1           NUM       compound  \ntrillion    NUM       nummod    \nmarket      NOUN      compound  \nvalue       NOUN      dobj      \n\n\nApple ORG\nfirst ORDINAL\nU.S. GPE\n$1 trillion MONEY\n"
     ]
    }
   ],
   "source": [
    "# Named Entities (predicted by statistical model)\n",
    "ner = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print('{:<12}{:<10}{:<10}'.format(token_text, token_pos, token_dep))\n",
    "\n",
    "print('\\n')\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple ORG\nMissing entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "text = \"New iPhone X release date leaked as Apple reveals pre-orders by mistake\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities\n",
    "for ent in doc.ents:\n",
    "    # print the entity text and label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Get the span for \"iPhone X\"\n",
    "iphone_x = doc[1:3]\n",
    "\n",
    "# Print the span text\n",
    "print('Missing entity:', iphone_x.text)"
   ]
  },
  {
   "source": [
    "## Pre-processing \n",
    "### Clean text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for cleaning text\n",
    "# no lemmatization or change of caps, as it could cange POS tag of a word.\n",
    "def clean(text):\n",
    "    # removing new line character\n",
    "    text = re.sub('\\n','', str(text))\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    # removing apostrophes\n",
    "    text = re.sub(\"'s\",'',str(text))\n",
    "    # removing hyphens\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"- \",'',str(text))\n",
    "    # removing quotation marks\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    return text\n"
   ]
  },
  {
   "source": [
    "Data I was given its more or less claned, so the above step could not be necessary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Split text into different sentences!\n",
    "Spliting text into differnet sentences will allow us to extract information from each sentence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['An engineer had to plan the construction of an artificial lake to produce electric energy.',\n",
       " 'To feed the lake he thought to build a unique wide canal collecting water coming from a near valley.',\n",
       " 'However, a mason pointed out that during the flood periods the stream of water flowing along the canal might be too strong and might damage the surrounding areas; by contrast, during the drought periods a unique stream of water might be insufficient to feed the lake.',\n",
       " 'In order to avoid these mishaps, the mason suggested to build, instead of a unique wide canal, four small canals whose total flow was the same as the unique wide canal previously planned.',\n",
       " 'These small canals were placed around the lake so that they conveyed water coming from four different valleys.',\n",
       " 'In this way only small amounts of water could flow in each canal and thus during flood periods dangerous overflowing might not occur.',\n",
       " 'At the same time, the lake was fed by water from various belts, so that also during drought periods it was sufficiency that the fed.']"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "# doc.sents is a generator that yields sentence spans\n",
    "[sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "An engineer had to plan the construction of an artificial lake to produce electric energy.\nTo feed the lake he thought to build a unique wide canal collecting water coming from a near valley.\nHowever, a mason pointed out that during the flood periods the stream of water flowing along the canal might be too strong and might damage the surrounding areas; by contrast, during the drought periods a unique stream of water might be insufficient to feed the lake.\nIn order to avoid these mishaps, the mason suggested to build, instead of a unique wide canal, four small canals whose total flow was the same as the unique wide canal previously planned.\nThese small canals were placed around the lake so that they conveyed water coming from four different valleys.\nIn this way only small amounts of water could flow in each canal and thus during flood periods dangerous overflowing might not occur.\nAt the same time, the lake was fed by water from various belts, so that also during drought periods it was sufficiency that the fed.\n"
     ]
    }
   ],
   "source": [
    "def sentence(text):\n",
    "    splitted = []\n",
    "    tokens = nlp(text)\n",
    "    for sent in tokens.sents:\n",
    "        splitted.append(sent.string.strip())\n",
    "        print(sent.string.strip())\n",
    "    return splitted\n",
    "splitted = sentence(text)"
   ]
  },
  {
   "source": [
    "## Base noun phrases (needs the tagger and parser)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['An engineer',\n",
       " 'the construction',\n",
       " 'an artificial lake',\n",
       " 'electric energy',\n",
       " 'the lake',\n",
       " 'he',\n",
       " 'a unique wide canal collecting water',\n",
       " 'a near valley',\n",
       " 'a mason',\n",
       " 'the flood periods',\n",
       " 'the stream',\n",
       " 'water',\n",
       " 'the canal',\n",
       " 'the surrounding areas',\n",
       " 'contrast',\n",
       " 'a unique stream',\n",
       " 'water',\n",
       " 'the lake',\n",
       " 'order',\n",
       " 'these mishaps',\n",
       " 'the mason',\n",
       " 'a unique wide canal',\n",
       " 'four small canals',\n",
       " 'whose total flow',\n",
       " 'the unique wide canal',\n",
       " 'These small canals',\n",
       " 'the lake',\n",
       " 'they',\n",
       " 'water',\n",
       " 'four different valleys',\n",
       " 'this way',\n",
       " 'only small amounts',\n",
       " 'water',\n",
       " 'each canal',\n",
       " 'flood periods dangerous overflowing',\n",
       " 'the same time',\n",
       " 'the lake',\n",
       " 'water',\n",
       " 'various belts',\n",
       " 'drought periods',\n",
       " 'it']"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "# doc = nlp(\"I have a brown car\")\n",
    "doc = nlp(text)\n",
    "# doc.noun_chunks is a generator that yields spans\n",
    "[chunk.text for chunk in doc.noun_chunks]"
   ]
  },
  {
   "source": [
    "## Label explanations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-86441f1a63d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ORG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PREP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "spacy.explain('NN')\n",
    "spacy.explain('GPE')\n",
    "spacy.explain('ORG')\n",
    "spacy.explain('PP')\n",
    "spacy.explain('PREP')"
   ]
  },
  {
   "source": [
    "## Information Extraction\n",
    "Noun-Verb-Noun Phrases "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for rule 1: noun(subject), verb, noun(object)\n",
    "def rule1(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sent = []\n",
    "\n",
    "    for token in doc:\n",
    "        # if the token is a verb\n",
    "        if (token.pos_=='VERB'):\n",
    "            phrase =''\n",
    "            # only extract noun or pronoun subjects\n",
    "            for sub_tok in token.lefts:\n",
    "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
    "                    # add subject to the phrase\n",
    "                    phrase += sub_tok.text\n",
    "                    # save the root of the verb in phrase\n",
    "                    phrase += ' '+token.lemma_ \n",
    "                    # check for noun or pronoun direct objects\n",
    "                    for sub_tok in token.rights:  \n",
    "                        # save the object in the phrase\n",
    "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):          \n",
    "                            phrase += ' '+sub_tok.text\n",
    "                            sent.append(phrase)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "engineer\nplan\nconstruction\nlake\nproduce\nenergy\n[engineer, plan, construction, lake, produce, energy]\n"
     ]
    }
   ],
   "source": [
    "# function for rule 1: noun(subject), verb, noun(object)\n",
    "def rule1(text):\n",
    "    \"\"\"\n",
    "     Returns noun(subject)-verb-noun(object) relation.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): Sinlge string with sentence\n",
    "                    \n",
    "\n",
    "            Returns:\n",
    "                    sent (str): Noun-verb-noun triple\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sent = []\n",
    "\n",
    "    for token in doc:\n",
    "        # print(token.text, token.pos_)\n",
    "        if token.pos_ == 'NOUN':\n",
    "            print(token)\n",
    "            sent.append(token)\n",
    "        if token.pos_ == 'VERB':\n",
    "            print(token)\n",
    "            sent.append(token)\n",
    "\n",
    "    print(sent)\n",
    "\n",
    "rule1('An engineer had to plan the construction of an artificial lake to produce electric energy.')\n"
   ]
  },
  {
   "source": [
    "## Visualizing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">An engineer had to plan the construction of an artificial lake to produce electric energy. To feed the lake he thought to build a unique wide canal collecting water coming from a near valley. However, a mason pointed out that during the flood periods the stream of water flowing along the canal might be too strong and might damage the surrounding areas; by contrast, during the drought periods a unique stream of water might be insufficient to feed the lake. In order to avoid these mishaps, the mason suggested to build, instead of a unique wide canal, \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    four\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n small canals whose total flow was the same as the unique wide canal previously planned. These small canals were placed around the lake so that they conveyed water coming from \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    four\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n different valleys. In this way only small amounts of water could flow in each canal and thus during flood periods dangerous overflowing might not occur. At the same time, the lake was fed by water from various belts, so that also during drought periods it was sufficiency that the \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    fed\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n.</div></span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "source": [
    "## Word vectors and similarity\n",
    "- To use word vectors, you need to install the larger models ending in md or lg , for example en_core_web_md. ## Comparing similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.881463757291801"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I like dogs\")\n",
    "# Compare 2 documents\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.73113453"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "# Compare 2 tokens\n",
    "doc1[2].similarity(doc2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.020106245"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "# Compare tokens and spans\n",
    "doc1[0].similarity(doc2[1:3])"
   ]
  },
  {
   "source": [
    "## Accessing word vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.7485528 , -0.5302772 , -0.28065848,  1.6679082 , -2.5105572 ,\n",
       "       -0.69197637,  2.3380795 ,  2.8330586 ,  0.24699098,  1.1241426 ,\n",
       "       -0.36723453,  1.8007439 , -3.748637  ,  1.5548012 ,  1.6259949 ,\n",
       "       -2.7272515 ,  0.8689956 , -4.114587  ,  0.99831045,  2.4935718 ,\n",
       "       -0.7050187 ,  0.04261035, -1.9289498 , -0.6119052 , -0.86213654,\n",
       "        2.5025008 , -1.6169056 , -2.8115811 ,  0.7515799 , -3.806582  ,\n",
       "        2.930271  , -3.2458625 ,  0.35243738,  2.603886  , -4.441632  ,\n",
       "        2.7368166 , -2.590376  ,  0.871832  , -1.7094355 ,  1.9728762 ,\n",
       "        1.351961  ,  3.2549381 ,  2.276856  , -0.27092087, -0.03417099,\n",
       "        0.3526801 ,  0.89299   ,  1.2564688 ,  0.16750032,  0.01128793,\n",
       "        6.106284  , -0.4530599 , -0.0590274 ,  0.9944489 ,  3.5443711 ,\n",
       "        1.2056826 ,  2.0902636 ,  1.393859  ,  1.638968  , -1.2946205 ,\n",
       "       -0.14186192, -5.1092973 , -3.9598703 , -1.2180623 , -3.8225663 ,\n",
       "        3.1138935 , -1.486633  , -1.4576025 ,  0.80167985, -2.527148  ,\n",
       "        1.593164  , -0.8516298 , -2.0259736 , -2.4401116 , -1.5462935 ,\n",
       "       -0.03334019,  1.0380609 ,  1.4740571 , -1.8475528 , -1.9712911 ,\n",
       "       -3.7562037 ,  0.9295623 ,  1.1091807 , -0.747375  , -1.7384481 ,\n",
       "       -0.39402407, -3.3554745 ,  1.2864517 ,  4.6326537 ,  3.9160528 ,\n",
       "       -2.3448977 ,  5.0466876 ,  1.5388513 , -0.70489275, -0.01551194,\n",
       "        0.1974119 ], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "# Vector as a numpy array\n",
    "doc = nlp(text)\n",
    "# The L2 norm of the token's vector\n",
    "doc[2].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21.839558"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "doc[2].vector_norm"
   ]
  },
  {
   "source": [
    "## Pipeline components\n",
    "Functions that take a Doc object, modify it and return it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Pipeline infrormation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x1234c0370>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x1234ccb80>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x1234cce20>)]"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "source": [
    "### Custom component"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that modifies the doc and returns it\n",
    "def custom_component(doc):\n",
    "    print(\"Do something to the doc here!\")\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)"
   ]
  },
  {
   "source": [
    "Components can be added first, last (default), or before or after an existing component."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Extension Attributes\n",
    "Custom attributes that are registered on the global Doc, Token and Span classes and become available as ._."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Do something to the doc here!\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc, Token, Span\n",
    "doc = nlp(\"The sky over Guwahati is blue\")"
   ]
  },
  {
   "source": [
    "## Attribute extensions (with default value)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register custom attribute on Token class\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "# Overwrite extension attribute with default value\n",
    "doc[5]._.is_color = True"
   ]
  },
  {
   "source": [
    "## Property Extension (with getter & setter)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'eulb si itahawuG revo yks ehT'"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "# Register custom attribute on Doc class\n",
    "get_reversed = lambda doc: doc.text[::-1]\n",
    "Doc.set_extension(\"reversed\", getter=get_reversed)\n",
    "# Compute value of extension attribute with getter\n",
    "doc._.reversed"
   ]
  },
  {
   "source": [
    "## Method extension (callable method)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "# Register custom attribute on Span class\n",
    "has_label = lambda span, label: span.label_ == label\n",
    "Span.set_extension(\"has_label\", method=has_label)\n",
    "# Compute value of extension attribute with method\n",
    "doc[3:5]._.has_label(\"GPE\")"
   ]
  },
  {
   "source": [
    "## Rule-based matching\n",
    "### Using the Matcher"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Do something to the doc here!\nNew York\n"
     ]
    }
   ],
   "source": [
    "# Matcher is initialized with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "# Each dict represents one token and its attributes\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add with ID, optional callback and pattern(s)\n",
    "pattern = [{\"LOWER\": \"new\"}, {\"LOWER\": \"york\"}]\n",
    "matcher.add('CITIES', None, pattern)\n",
    "# Match by calling the matcher on a Doc object\n",
    "doc = nlp(\"I live in New York\")\n",
    "matches = matcher(doc)\n",
    "# Matches are (match_id, start, end) tuples\n",
    "for match_id, start, end in matches:\n",
    "     # Get the matched span by slicing the Doc\n",
    "     span = doc[start:end]\n",
    "     print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Do something to the doc here!\nTotal matches found: 3\nMatch found: iOS 7\nMatch found: iOS 11\nMatch found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"After making the iOS update you won't notice a radical system-wide redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of iOS 11's furniture remains the same as in iOS 10. But you will discover some tweaks once you delve a little deeper.\")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{'TEXT': 'iOS'}, {'IS_DIGIT': True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('IOS_VERSION_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Do something to the doc here!\nTotal matches found: 2\nMatch found: downloaded Fortnite\nMatch found: downloading Minecraft\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"i downloaded Fortnite on my laptop and can't open the game at all. Help? so when I was downloading Minecraft, I got the Windows version where it is the '.zip' folder and I used the default program to unpack it... do I also need to download Winzip?\")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{'LEMMA': 'download'}, {'POS': 'PROPN'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('DOWNLOAD_THINGS_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Do something to the doc here!\nTotal matches found: 5\nMatch found: beautiful design\nMatch found: smart search\nMatch found: automatic labels\nMatch found: optional voice\nMatch found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('ADJ_NOUN_PATTERN', None, pattern)\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "source": [
    "### Token patterns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"love cats\", \"loving cats\", \"loved cats\"\n",
    "pattern1 = [{\"LEMMA\": \"love\"}, {\"LOWER\": \"cats\"}]\n",
    "# \"10 people\", \"twenty people\"\n",
    "pattern2 = [{\"LIKE_NUM\": True}, {\"TEXT\": \"people\"}]\n",
    "# \"book\", \"a cat\", \"the sea\" (noun + optional article)\n",
    "pattern3 = [{\"POS\": \"DET\", \"OP\": \"?\"}, {\"POS\": \"NOUN\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}