{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import scispacy\n",
    "from Triple import Triple\n",
    "from tqdm import tqdm\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_TEXT = \"Cardiovascular Magnetic Resonance Findings in Competitive Athletes Recovering From COVID-19 Infection Myocarditis is a significant cause of sudden cardiac death in competitive athletes and can occur with normal ventricular function.1 Recent studies have raised concerns of myocardial inflammation after recovery from coronavirus disease 2019 (COVID-19), even in asymptomatic or mildly symptomatic patients.2 Our objective was to investigate the use of cardiac magnetic resonance (CMR) imaging in competitive athletes recovered from COVID-19 to detect myocardial inflammation that would identify high-risk athletes for return to competitive play. We performed a comprehensive CMR examination including cine, T1 and T2 mapping, extracellular volume fraction, and late gadolinium enhancement (LGE), on a 1.5-T scanner (Magnetom Sola; Siemens Healthineers) using standardized protocols,3 in all competitive athletes referred to the sports medicine clinic after testing positive for COVID-19 (reverse transcriptase–polymerase chain reaction) between June and August 2020. The Ohio State University institutional review board approved the study, and informed consent in writing was obtained from participating athletes. Cardiac magnetic resonance imaging was performed after recommended quarantine (11-53 days). Electrocardiogram, serum troponin I, and transthoracic echocardiogram were performed on day of CMR imaging. We performed CMR imaging in 26 competitive college athletes (mean [SD] age, 19.5 [1.5] years; 15 male [57.7%]) from the following sports: football, soccer, lacrosse, basketball, and track. No athletes required hospitalization or received COVID-19–specific antiviral therapy. Twelve athletes (26.9%; including 7 female individuals) reported mild symptoms during the short-term infection (sore throat, shortness of breath, myalgias, fever), while others were asymptomatic. There were no diagnostic ST/T wave changes on electrocardiogram, and ventricular volumes and function were within the normal range in all athletes by transthoracic echocardiogram and CMR imaging. No athlete had elevated serum levels of troponin I. Four athletes (15%; all male individuals) had CMR findings consistent with myocarditis based on the presence of 2 main features of the updated Lake Louise Criteria: myocardial edema by elevated T2 signal and myocardial injury by presence of nonischemic LGE (~Figure~).4 Pericardial effusion was present in 2 athletes with CMR evidence of myocarditis. Two of these 4 athletes with evidence of myocardial inflammation had mild symptoms (shortness of breath), while the other 2 were asymptomatic. Twelve athletes (46%) had LGE (mean of 2 American Heart Association segments), of whom 8 (30.8%) had LGE without concomitant T2 elevation (~Table~). Mean (SD) T2 in those with suspected myocarditis was 59 (3) milliseconds compared with 51 (2) milliseconds in those without CMR evidence of myocarditis. Of 26 competitive athletes, 4 (15%) had CMR findings suggestive of myocarditis and 8 additional athletes (30.8%) exhibited LGE without T2 elevation suggestive of prior myocardial injury. COVID-19–related myocardial injury in competitive athletes and sports participation remains unclear. Cardiac magnetic resonance imaging has the potential to identify a high-risk cohort for adverse outcomes and may, importantly, risk stratify athletes for safe participation because CMR mapping techniques have a high negative predictive value to rule out myocarditis.4 A recent study by Puntmann et al2 demonstrated cardiac involvement in a significant number of patients who had recovered from COVID-19. A recent expert consensus article recommended 2-week convalescence followed by no diagnostic cardiac testing if asymptomatic and an electrocardiogram and transthoracic echocardiogram in mildly symptomatic athletes with COVID-19 to return to play for competitive sports.5 However, emerging knowledge and CMR observations question this recommendation. Cardiac magnetic resonance imaging evidence of myocardial inflammation has been associated with poor outcomes, including myocardial dysfunction and mortality.6 Study limitations include lack of baseline CMR imaging and variable timing of CMR imaging from a positive COVID-19 test result. Athletic cardiac adaptation could be responsible for these abnormalities; however, in this cohort, mean (SD) T2 in those with suspected myocarditis was 59 (3) milliseconds vs 51 (2) milliseconds in those without, favoring pathology. Additionally, the rate of LGE (42%) is higher than in previously described normative populations. To conclude, while long-term follow-up and large studies including control populations are required to understand CMR changes in competitive athletes, CMR may provide an excellent risk-stratification assessment for myocarditis in athletes who have recovered from COVID-19 to guide safe competitive sports participation.\"\n",
    "modifiers = \"The man with black beard was giving the speech.\"\n",
    "mod02 = \"We saw the man when we were entering the room.\"\n",
    "person01 = \"Lucy like big suprises.\"\n",
    "doc = nlp(person01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before [('Mike', 0, 4, 'PERSON'), ('Lucy', 10, 14, 'PERSON')]\n",
      "Mike PERSON\n",
      "We got 1 in ['Mike']\n",
      "Lucy PERSON\n",
      "We got 2 in ['Mike', 'Lucy']\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Mike', 'Lucy']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities.new_entities(\"Mike like Lucy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lucy PERSON\n",
      "this is my person Lucy\n"
     ]
    }
   ],
   "source": [
    "for item in doc.ents:\n",
    "    print(item.text,item.label_)\n",
    "    if item.label_ == 'PERSON':\n",
    "        print(f\"this is my person {item.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '']\n",
      "Before [('Mike', 0, 4, 'PERSON'), ('Lucy', 10, 14, 'PERSON')]\n",
      "Mike PERSON\n",
      "We got 2 in ['Mike', 'Mike']\n",
      "Lucy PERSON\n",
      "We got 4 in ['Mike', 'Mike', 'Lucy', 'Lucy']\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Mike', 'Lucy']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(entities.entities(\"Mike and Lucy\"))\n",
    "# \"fb is hiring a new vice president of global policy\"\n",
    "# \"Susan and July love red wine.\"\n",
    "entities.new_entities(\"Mike like Lucy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import sent_\n",
    "sent_(mod02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Mark crosed the street really fast.\")\n",
    "spacy.displacy.render(doc, style=\"ent\")\n",
    "spacy.displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triple(self):\n",
    "    doc = nlp(self.text)\n",
    "    sent = []\n",
    "    reldeps = [\"ROOT\", \"adj\", \"attr\", \"agent\", \"amod\"]\n",
    "    # TODO: change doc to nlp()\n",
    "    for token in doc:\n",
    "        # if the token is a verb\n",
    "        if (token.pos_ in ['VERB','ROOT']):\n",
    "            phrase = ''\n",
    "            # only extract noun or pronoun subjects\n",
    "            for sub_tok in token.lefts:\n",
    "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
    "                    # add subject to the phrase\n",
    "                    phrase += sub_tok.text\n",
    "                    # save the root of the verb in phrase\n",
    "                    phrase += ' '+token.lemma_\n",
    "                    # check for noun or pronoun direct objects\n",
    "                    for sub_tok in token.rights:\n",
    "                        # save the object in the phrase\n",
    "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
    "                            phrase += ' '+sub_tok.text\n",
    "                            sent.append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "# nlp = spacy.load('en_core_sci_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder = '/Users/awenc/NUIM/CS440/KG_NLPSystem/data/Psychology Test Materials'\n",
    "path_to_csv_output = '/Users/awenc/NUIM/CS440/KG_NLPSystem/workspace/sentences_psychology.csv'\n",
    "fileconvert(path_to_folder, path_to_csv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = \"/Users/awenc/NUIM/CS440/KG_NLPSystem/data/sentences.csv\"\n",
    "candidate_sentences = pd.read_csv(path_to_csv)\n",
    "path_to_csv = \"/Users/awenc/NUIM/CS440/KG_NLPSystem/workspace/sentences_psychology.csv\"\n",
    "psycho_sentences = pd.read_csv(path_to_csv)\n",
    "print(len(psycho_sentences['sentences']))\n",
    "print(len(candidate_sentences['sentences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs = []\n",
    "relations = []\n",
    "for i in tqdm(psycho_sentences['sentences']):\n",
    "    entity_pairs.append(get_entities(i))\n",
    "    relations.append(get_relation(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=Triple(psycho_sentences['sentences'][5]).get_entities()\n",
    "b=Triple(psycho_sentences['sentences'][5]).get_relation()\n",
    "c = Triple(psycho_sentences['sentences'][5]).get_triple()\n",
    "f\"{a[0]} {b} {a[1]} Triple: {c}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_ = [relation(i) for i in psycho_sentences['sentences']]\n",
    "object_ = [i[0] for i in entity_pairs]\n",
    "subject_ = [i[1] for i in entity_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['VERB','ROOT']\n",
    "\n",
    "def get_triple(self):\n",
    "    doc = nlp(self.text)\n",
    "    sent = []\n",
    "    # Tags relations dependencies\n",
    "    reldeps = [\"ROOT\", \"adj\", \"attr\", \"agent\", \"amod\"]\n",
    "    # Tags for entities(subjects and objects)\n",
    "    deps = [\"compound\", \"prep\", \"conj\", \"mod\"]\n",
    "    # TODO: change doc to nlp()\n",
    "    for token in doc:\n",
    "        # if the token is a verb\n",
    "        if (token.pos_ in reldeps):\n",
    "            phrase = ''\n",
    "            # only extract noun or pronoun subjects\n",
    "            for sub_tok in token.lefts:\n",
    "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
    "                    # add subject to the phrase\n",
    "                    phrase += sub_tok.text\n",
    "                    # save the root of the verb in phrase\n",
    "                    phrase += ' '+token.lemma_\n",
    "                    # check for noun or pronoun direct objects\n",
    "                    for sub_tok in token.rights:\n",
    "                        # save the object in the phrase\n",
    "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
    "                            phrase += ' '+sub_tok.text\n",
    "                            sent.append(phrase)\n",
    "\n",
    "    # print(sent)\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_(sentence):\n",
    "    \"\"\"\n",
    "    Analyse sentence, breaks sentence into: text, pos, dep\n",
    "    part of speach\n",
    "    dependency\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sent = nlp(sentence)\n",
    "    for token in sent:\n",
    "        # Get the token text, part-of-speech tag and dependency label\n",
    "        token_text = token.text\n",
    "        token_pos = token.pos_\n",
    "        token_dep = token.dep_\n",
    "        # This is for formatting only\n",
    "        x = '{:<12}{:<10}{:<10}{:<10}'.format(token_text, token_pos, token_dep,spacy.explain(token_pos))\n",
    "        print(x)\n",
    "        # return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def output_triple_sent_():\n",
    "    i = psycho_sentences['sentences'][1]\n",
    "    so=get_entities(i)\n",
    "    v=get_relation(i)\n",
    "    triple=get_triple(i)\n",
    "    svo = so[0],v,so[1]\n",
    "    print('{} {}'.format(svo,triple))\n",
    "    print(f\"SVO: {svo} TRIPLE: {triple}\")\n",
    "    sent_(f\"{svo} {triple}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_triple():\n",
    "\n",
    "    outfile = open('./output/_psychology_spacy_output.txt','a')\n",
    "    for i in tqdm(psycho_sentences['sentences']):\n",
    "        so=get_entities(i)\n",
    "        v=get_relation(i)\n",
    "        triple=get_triple(i)\n",
    "        svo = so[0],v,so[1]\n",
    "        # print('{} {}'.format(svo,triple))\n",
    "        # print(f\"SVO: {svo} TRIPLE:{triple}\")\n",
    "        outfile.write(i)\n",
    "        outfile.write('\\n')\n",
    "        outfile.write(f\"SVO: {svo} TRIPLE:{triple}\")\n",
    "        outfile.write('\\n')\n",
    "    outfile.close()\n",
    "\n",
    "def to_file_sent_():\n",
    "    outfile = open('./psychology_sentences.txt','w')\n",
    "    for item in psycho_sentences['sentences']:\n",
    "        # print(sent_(item))\n",
    "    # print(\"\\n\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            outfile.write(sent_(item))\n",
    "            outfile.write('\\n')\n",
    "        except TypeError as e:\n",
    "            pass\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_(\"Because of their wellknown stance as activists, many members of the faculty have been called to testify before Congress.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This era has clearly left its mark on the faculty.\")\n",
    "print(doc[0])\n",
    "for i,tok in enumerate(doc):\n",
    "    if tok.dep_ in ['VERB','ROOT']:\n",
    "        loc=i\n",
    "        print(f\"{tok} {tok.dep_} {tok.pos_}\")\n",
    "        \n",
    "        print(f\"{doc[i]} {doc[i-1].pos_}\")\n",
    "        if doc[i-1].pos_ in ['ADV']:\n",
    "            print(f\"{doc[i-1]} {doc[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(token.text, token.i - sent.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        if token.dep_ in ['ROOT']:\n",
    "        #['advmod']:\n",
    "            print(token.text, token.i - sent.start)\n",
    "            print()\n",
    "            print(x for x in doc[token.i-1].subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(\"This era has clearly left its mark on the faculty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs = []\n",
    "relations = []\n",
    "for i in tqdm(psycho_sentences['sentences']):\n",
    "    entity_pairs.append(get_entities(i))\n",
    "    relations.append(get_relation(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(relations)):\n",
    "    print(f\"{entity_pairs[i][0]} {relations[i]} {entity_pairs[i][1]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_(\"I want to be a good writer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This era has clearly left its mark on the faculty.\")\n",
    "for np in doc.noun_chunks:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import *\n",
    "\n",
    "np_labels = set([nsubj, nsubjpass, dobj, iobj, pobj]) # Probably others too\n",
    "def iter_nps(doc):\n",
    "    for word in doc:\n",
    "        if word.dep in np_labels:\n",
    "            yield word.subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for np_label in iter_nps(doc): \n",
    "    print(next(np_label))\n",
    "\n",
    "def iter_nps(doc):\n",
    "    for word in doc:\n",
    "        if word.dep in np_labels:\n",
    "            print(word.text, word.dep_)\n",
    "\n",
    "iter_nps(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realtion extraction!\n",
    "\n",
    "import spacy   \n",
    "from spacy.matcher import Matcher\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "sentence = 'The cat sat on the mat. He quickly ran to the market. The dog jumped into the water. The author is writing a book.'\n",
    "sentence = 'This era has clearly left its mark on the faculty.'\n",
    "pattern = [{'POS': 'VERB', 'OP': '?'},\n",
    "           {'POS': 'ADV', 'OP': '*'},\n",
    "           {'POS': 'AUX', 'OP': '*'},\n",
    "           {'POS': 'VERB', 'OP': '+'}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities\n",
    "for x in doc:\n",
    "    if x.pos_ in ['NOUN','PROPN','PRON']:\n",
    "        # print(type(x.text[:]))\n",
    "        print(x.text)\n",
    "\n",
    "def entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]\n",
    "\n",
    "from spacy.util import filter_spans  \n",
    "def relation(sentence):\n",
    "    \"\"\"\n",
    "    Get relation within input sentenceence based on pattern provided.\n",
    "    params: str - input of single sentenceence.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Matcher class object\n",
    "    matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Match 0 or more times / match 0 or 1 time(one relation in sencence?)\n",
    "    Dodałem PROPN ale jeszcze nie przetestowałem.\n",
    "    \"\"\"\n",
    "    pattern0=[{'POS': 'VERB', 'OP': '?'},\n",
    "            {'POS': 'ADV', 'OP': '*'},\n",
    "            {'OP': '*'}, # additional wildcard - match any text in between\n",
    "            {'POS': 'VERB', 'OP': '+'}]\n",
    "    pattern1 = [{'DEP':'ROOT'},\n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},\n",
    "            {'POS':'PROPN','OP':'?'},\n",
    "            {'POS':'ADJ','OP':\"?\"}]\n",
    "    pattern = [{'POS': 'VERB', 'OP': '?'},\n",
    "           {'POS': 'ADV', 'OP': '*'},\n",
    "           {'POS': 'AUX', 'OP': '*'},\n",
    "           {'POS': 'VERB', 'OP': '+'}]\n",
    "    pattern2 = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}] \n",
    "    # pattern = [{'POS': 'VERB', 'OP': '?'}, {'POS': 'ADV', 'OP': ''}, {'OP': ''}, {'POS': 'VERB', 'OP': '+'}]\n",
    "\n",
    "    matcher.add(\"Verb phrase\", [pattern,pattern2,pattern0])\n",
    "\n",
    "    # call the matcher to find matches \n",
    "    matches = matcher(doc)\n",
    "    spans = [doc[start:end] for _, start, end in matches]\n",
    "\n",
    "    res = filter_spans(spans)\n",
    "    return res\n",
    "\n",
    "\n",
    "def entities2(sentence):\n",
    "    # Entities\n",
    "    doc = nlp(sentence)\n",
    "    for x in doc:\n",
    "        if x.pos_ in ['NOUN','PROPN','PRON']:\n",
    "            # print(type(x.text[:]))\n",
    "            # print(x.text)\n",
    "            return x.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs = []\n",
    "relations = []\n",
    "for i in tqdm(psycho_sentences['sentences']):\n",
    "    entity_pairs.append(entities(i))\n",
    "    relations.append(relation(i))\n",
    "\n",
    "entity_pairs[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_df = pd.DataFrame({'source':object_, 'target':subject_, 'edge':relations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(relations).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'London is the capital of England. Westminster is located in London.'\n",
    "print(relation(sentence))\n",
    "print(entities(sentence))\n",
    "print(entities2(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(entity_pairs))\n",
    "print(len(relations))\n",
    "for i in range(len(relations)//100):\n",
    "    print(f\"{entity_pairs[i][0]} {relations[i]} {entity_pairs[i][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "\n",
    "  doc = nlp(sent)\n",
    "\n",
    "  # Matcher class object \n",
    "  matcher = Matcher(nlp.vocab)\n",
    "\n",
    "  #define the pattern \n",
    "  pattern = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}] \n",
    "\n",
    "  matcher.add(\"matching_1\", [pattern]) \n",
    "\n",
    "  matches = matcher(doc)\n",
    "  k = len(matches) - 1\n",
    "\n",
    "  span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "  return(span.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPHING SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(next(doc.sents), style='dep')\n",
    "displacy.serve(next(doc.sents), style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kg_df = pd.DataFrame({'source':object_, 'target':subject_, 'edge':relations})\n",
    "# create a directed-graph from a dataframe\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"was\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specifica relation.\n",
    "\n",
    "G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"was\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "import visualise_spacy_tree\n",
    "png = visualise_spacy_tree.create_png(doc)\n",
    "\n",
    "# Write it to a file\n",
    "with open('parse_tree2.png', 'wb') as f:\n",
    "    f.write(png)\n",
    "\n",
    "# If you're using Jupyter notebook, you can render it inline\n",
    "from IPython.display import Image, display\n",
    "display(Image(png))\n",
    "\n",
    "# Override node attributes to customise the plot\n",
    "from spacy.tokens import Token\n",
    "Token.set_extension('plot', default={}, force=True)  # Create a token underscore extension\n",
    "for token in doc:\n",
    "    node_label = '{0} [{1}])'.format(token.orth_, token.i)\n",
    "    token._.plot['label'] = node_label\n",
    "    if token.dep_ == 'ROOT':\n",
    "        token._.plot['color'] = 'green'\n",
    "\n",
    "'''\n",
    "Dependency Tree of the sentence.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define to file\n",
    "# function for entering object/list and saving it to a file.\n",
    "\n",
    "# create a file \n",
    "# open file \n",
    "# write to file or append\n",
    "# save file \n",
    "# close file \n",
    "\n",
    "\n",
    "def to_file(list_, path_to_file):\n",
    "    outfile = open(str(path_to_file),'a')\n",
    "    for item in list_:\n",
    "        outfile.write(item)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "    outfile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('kgsys2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4ef060659159fbeb4441d8e36b456b29bba2f0d8bf3938743d35f5235f44b89c"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}