{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import spacy\n",
    "import scispacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "# import neuralcoref\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create triple class\n",
    "import spacy\n",
    "import sys\n",
    "from spacy.matcher import Matcher\n",
    "from spacy import displacy\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "# this is invisible for class?\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# TODO: rework get entitirs\n",
    "\n",
    "class Triple:\n",
    "    \"\"\"\n",
    "    A class to combine all components of pipeline\n",
    "    for extracting triple from sentence, but probably it doesnt make\n",
    "    sense to use class for that, but well why the hell not?\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    attr0 : str\n",
    "        first name of the triple\n",
    "    attr1 : str\n",
    "        family name of the triple\n",
    "    attr2 : int\n",
    "        age of the triple\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    get_relation\n",
    "    get_entities\n",
    "    get_triple()\n",
    "    info(additional=\"\"):\n",
    "        Prints the triple's name and age.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the triple object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        SVO - Subject - verb - object\n",
    "            text : str\n",
    "                sentence or block of text\n",
    "            subject : str\n",
    "                subject of the sentence\n",
    "            relation : str\n",
    "                relation between subject and object\n",
    "            object : str\n",
    "                object or objects of the text or the sentence\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        global nlp\n",
    "        global doc\n",
    "        doc = nlp(self.text)\n",
    "\n",
    "    # TODO: CHYBA DZIALA BEZ SETUP\n",
    "    # def setup(self):\n",
    "        # global nlp\n",
    "        # global doc\n",
    "        # nlp = spacy.load('en_core_web_sm')\n",
    "        # doc = nlp(self.text)\n",
    "\n",
    "\n",
    "    def relation(sentence):\n",
    "        \"\"\"\n",
    "        Get relation within input sentenceence based on pattern provided.\n",
    "        params: str - input of single sentenceence.\n",
    "        \"\"\"\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        # Matcher class object\n",
    "        matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "        \"\"\"\n",
    "        Match 0 or more times / match 0 or 1 time(one relation in sencence?)\n",
    "        Dodałem PROPN ale jeszcze nie przetestowałem.\n",
    "        \"\"\"\n",
    "        pattern0=[{'POS': 'VERB', 'OP': '?'},\n",
    "                {'POS': 'ADV', 'OP': '*'},\n",
    "                {'OP': '*'}, # additional wildcard - match any text in between\n",
    "                {'POS': 'VERB', 'OP': '+'}]\n",
    "        pattern1 = [{'DEP':'ROOT'},\n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},\n",
    "                {'POS':'PROPN','OP':'?'},\n",
    "                {'POS':'ADJ','OP':\"?\"}]\n",
    "        pattern = [{'POS': 'VERB', 'OP': '?'},\n",
    "            {'POS': 'ADV', 'OP': '*'},\n",
    "            {'POS': 'AUX', 'OP': '*'},\n",
    "            {'POS': 'VERB', 'OP': '+'}]\n",
    "        pattern2 = [{'DEP':'ROOT'},\n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},\n",
    "                {'POS':'ADJ','OP':\"?\"}]\n",
    "        # pattern = [{'POS': 'VERB', 'OP': '?'}, {'POS': 'ADV', 'OP': ''}, {'OP': ''}, {'POS': 'VERB', 'OP': '+'}]\n",
    "        # for matcher.add had to change for 2 arguments and second as list\n",
    "        # it happen after i had to downgrade python version to python=3.6\n",
    "        # now it gets 2 arguments instead of 3, latter is type list so we can\n",
    "        # add multiple patterns.\n",
    "        # # instantiate a Matcher instance\n",
    "        # matcher = Matcher(nlp.vocab)\n",
    "        matcher.add(\"Verb phrase\", [pattern,pattern2,pattern0])\n",
    "\n",
    "        # call the matcher to find matches\n",
    "        matches = matcher(doc)\n",
    "        spans = [doc[start:end] for _, start, end in matches]\n",
    "\n",
    "        res = filter_spans(spans)\n",
    "        return res\n",
    "\n",
    "    def get_triple(self):\n",
    "        doc = nlp(self.text)\n",
    "        sent = []\n",
    "        reldeps = [\"ROOT\", \"adj\", \"attr\", \"agent\", \"amod\"]\n",
    "        # TODO: change doc to nlp()\n",
    "        for token in doc:\n",
    "            # if the token is a verb\n",
    "            if (token.pos_ in ['VERB','ROOT']):\n",
    "                phrase = ''\n",
    "                # only extract noun or pronoun subjects\n",
    "                for sub_tok in token.lefts:\n",
    "                    if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
    "                        # add subject to the phrase\n",
    "                        phrase += sub_tok.text\n",
    "                        # save the root of the verb in phrase\n",
    "                        phrase += ' '+token.lemma_\n",
    "                        # check for noun or pronoun direct objects\n",
    "                        for sub_tok in token.rights:\n",
    "                            # save the object in the phrase\n",
    "                            if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
    "                                phrase += ' '+sub_tok.text\n",
    "                                sent.append(phrase)\n",
    "\n",
    "        # print(sent)\n",
    "        return sent\n",
    "\n",
    "    def entities(self.text):\n",
    "        entity1 = \"\"\n",
    "        entity2 = \"\"\n",
    "\n",
    "        prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "        prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        person = \"\"\n",
    "        persons = []\n",
    "\n",
    "        doc = nlp(self.text)\n",
    "        ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "        print('Entities', ents)\n",
    "        for item in doc.ents:\n",
    "            # print(item.text,item.label_)\n",
    "\n",
    "            if item.label_ == 'PERSON':\n",
    "                # look for person entities.\n",
    "                persons.append(str(item.text))\n",
    "\n",
    "            print(f\"We got {len(persons)} in {persons}\")\n",
    "\n",
    "        for token in doc:\n",
    "            # igonre punctuation\n",
    "            if token.dep_ != 'punkt':\n",
    "                # include compoound words\n",
    "                if token.dep_ == 'compound':\n",
    "                    prefix = token.text\n",
    "                    if prv_tok_dep == 'compound':\n",
    "                        prefix = prv_tok_text + \" \" + token.text\n",
    "                # check if token is a modifier\n",
    "                if token.dep_.endswith('mod') == True:\n",
    "                    modifier = token.text\n",
    "                    if prv_tok_dep == 'compound':\n",
    "                        modifier = prv_tok_text + \" \" + token.text\n",
    "                # find any form/kind of subject\n",
    "                if token.dep_.find('subj') == True:\n",
    "                    # create entity1, subject\n",
    "                    entity1 = modifier + \" \" + prefix + \" \" + token.text\n",
    "                    # reset variables\n",
    "                    prefix = \"\"\n",
    "                    modifier = \"\"\n",
    "                    prv_tok_dep = \"\"\n",
    "                    prv_tok_text = \"\"\n",
    "\n",
    "                if token.dep_.find('obj') == True:\n",
    "                    entity2 = modifier + \" \" + prefix + \" \" + token.text\n",
    "\n",
    "                # update variables\n",
    "                prv_tok_dep = token.dep_\n",
    "                prv_tok_text = token.text\n",
    "\n",
    "            # If subject not captured use person entity\n",
    "            if entity1.strip() == '':\n",
    "                entity1 = persons[0]\n",
    "                # if object not captured use other\n",
    "                if entity2.strip() == '':\n",
    "                    if len(persons) > 1:\n",
    "                        entity2 = persons[1]\n",
    "                    else:\n",
    "                        entity2 = modifier + \" \" + prefix + \" \" + token.text\n",
    "        \n",
    "        return [entity1, entity2.strip()]\n",
    "\n",
    "    def graph0(self):\n",
    "        doc = nlp(self.text)\n",
    "        displacy.render(doc, style='dep')\n",
    "\n",
    "    def graph1():\n",
    "        pass\n",
    "    def graph2():\n",
    "        pass\n",
    "\n",
    "    def set_doc(self,text):\n",
    "        doc = nlp(self.text)\n",
    "        return doc\n",
    "\n",
    "    def set_model(self,model):\n",
    "        nlp = spacy.load(model)\n",
    "        return nlp\n",
    "\n",
    "    def info(self, additional=\"\"):\n",
    "        \"\"\"\n",
    "        Prints the person's name and age.\n",
    "\n",
    "        If the argument 'additional' is passed, then it is appended after the main info.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        additional : str, optional\n",
    "            More info to be displayed (default is None)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        # print(f'My name is {self.text} {self.nlp}. I am {self.age} years old.' + additional)\n",
    "\n",
    "    # get_relation()\n",
    "    # get_triple()\n",
    "    # get_entities()\n",
    "    # displacy.serve(next(doc.sents), style='dep')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Triple import Triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Arthur', 0, 6, 'PERSON'), ('Martin', 16, 22, 'ORG')]\n",
      "2\n",
      "2\n",
      "['Arthur']\n",
      "my persons  ['Arthur']\n"
     ]
    },
    {
     "data": {
      "text/plain": "['', 'garden']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Triple('Arthur was with Martin in garden').entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9bca39033d7a43f5b177f663f069532d-0\" class=\"displacy\" width=\"575\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Mike</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">likes</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">July</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n</text>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-9bca39033d7a43f5b177f663f069532d-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-9bca39033d7a43f5b177f663f069532d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-9bca39033d7a43f5b177f663f069532d-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-9bca39033d7a43f5b177f663f069532d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M400.0,91.5 L408.0,79.5 392.0,79.5\" fill=\"currentColor\"/>\n</g>\n</svg></span>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Triple(\"Mike likes July\").graph0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtree_matcher(doc):\n",
    "  subjpass = 0\n",
    "\n",
    "  for i,tok in enumerate(doc):\n",
    "    # find dependency tag that contains the text \"subjpass\"    \n",
    "    if tok.dep_.find(\"subjpass\") == True:\n",
    "      subjpass = 1\n",
    "\n",
    "  x = ''\n",
    "  y = ''\n",
    "\n",
    "  # if subjpass == 1 then sentence is passive\n",
    "  if subjpass == 1:\n",
    "    for i,tok in enumerate(doc):\n",
    "      if tok.dep_.find(\"subjpass\") == True:\n",
    "        y = tok.text\n",
    "\n",
    "      if tok.dep_.endswith(\"obj\") == True:\n",
    "        x = tok.text\n",
    "  \n",
    "  # if subjpass == 0 then sentence is not passive\n",
    "  else:\n",
    "    for i,tok in enumerate(doc):\n",
    "      if tok.dep_.endswith(\"subj\") == True:\n",
    "        x = tok.text\n",
    "\n",
    "      if tok.dep_.endswith(\"obj\") == True:\n",
    "        y = tok.text\n",
    "\n",
    "  return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags I've chosen for relations\n",
    "deps = [\"ROOT\", \"adj\", \"attr\", \"agent\", \"amod\"]\n",
    "\n",
    "# Tags I've chosen for entities(subjects and objects)\n",
    "deps = [\"compound\", \"prep\", \"conj\", \"mod\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sent_(text):\n",
    "    \"\"\"\n",
    "    Function prints out attributes of word from spacy.\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : str\n",
    "            Block of text with multiple sentences\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    Prints attributes of token object.\n",
    "    text: Get the token text.\n",
    "    POS: part-of-speech tag.\n",
    "    DEP: dependency label.\n",
    "    \"\"\"\n",
    "    sent = nlp(text)\n",
    "    for token in sent:\n",
    "        # Get the token text, part-of-speech tag and dependency label\n",
    "        token_text = token.text\n",
    "        token_pos = token.pos_\n",
    "        token_dep = token.dep_\n",
    "        print('{:<12}{:<10}{:<10}{:<10}'.format(token_text, token_pos, token_dep,spacy.explain(token_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n",
    "def getSentences(text):\n",
    "    nlp = English()\n",
    "    nlp.add_pipe('sentencizer')\n",
    "    document = nlp(text)\n",
    "    return [sent.text for sent in document.sents]\n",
    "\n",
    "def printToken(token):\n",
    "    print(token.text, \"->\", token.dep_)\n",
    "\n",
    "def appendChunk(original, chunk):\n",
    "    return original + ' ' + chunk\n",
    "\n",
    "def isRelationCandidate(token):\n",
    "    deps = [\"ROOT\", \"adj\", \"attr\", \"agent\", \"amod\"]\n",
    "    return any(subs in token.dep_ for subs in deps)\n",
    "\n",
    "def isConstructionCandidate(token):\n",
    "    deps = [\"compound\", \"prep\", \"conj\", \"mod\"]\n",
    "    return any(subs in token.dep_ for subs in deps)\n",
    "\n",
    "def processSubjectObjectPairs(tokens):\n",
    "    subject = ''\n",
    "    object = ''\n",
    "    relation = ''\n",
    "    subjectConstruction = ''\n",
    "    objectConstruction = ''\n",
    "    for token in tokens:\n",
    "        printToken(token)\n",
    "        if \"punct\" in token.dep_:\n",
    "            continue\n",
    "        if isRelationCandidate(token):\n",
    "            relation = appendChunk(relation, token.lemma_)\n",
    "        if isConstructionCandidate(token):\n",
    "            if subjectConstruction:\n",
    "                subjectConstruction = appendChunk(subjectConstruction, token.text)\n",
    "            if objectConstruction:\n",
    "                objectConstruction = appendChunk(objectConstruction, token.text)\n",
    "        if \"subj\" in token.dep_:\n",
    "            subject = appendChunk(subject, token.text)\n",
    "            subject = appendChunk(subjectConstruction, subject)\n",
    "            subjectConstruction = ''\n",
    "        if \"obj\" in token.dep_:\n",
    "            object = appendChunk(object, token.text)\n",
    "            object = appendChunk(objectConstruction, object)\n",
    "            objectConstruction = ''\n",
    "\n",
    "    print (subject.strip(), \",\", relation.strip(), \",\", object.strip())\n",
    "    return (subject.strip(), relation.strip(), object.strip())\n",
    "\n",
    "def processSentence(sentence):\n",
    "    tokens = nlp_model(sentence)\n",
    "    return processSubjectObjectPairs(tokens)\n",
    "\n",
    "def printGraph(triples):\n",
    "    G = nx.Graph()\n",
    "    for triple in triples:\n",
    "        G.add_node(triple[0])\n",
    "        G.add_node(triple[1])\n",
    "        G.add_node(triple[2])\n",
    "        G.add_edge(triple[0], triple[1])\n",
    "        G.add_edge(triple[1], triple[2])\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "    plt.figure()\n",
    "    nx.draw(G, pos, edge_color='black', width=1, linewidths=1,\n",
    "            node_size=500, node_color='seagreen', alpha=0.9,\n",
    "            labels={node: node for node in G.nodes()})\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    text = \"London is the capital and largest city of England and the United Kingdom. Standing on the River \" \\\n",
    "           \"Thames in the south-east of England, at the head of its 50-mile (80 km) estuary leading to \" \\\n",
    "           \"the North Sea, London has been a major settlement for two millennia. \" \\\n",
    "           \"Londinium was founded by the Romans. The City of London, \" \\\n",
    "           \"London's ancient core − an area of just 1.12 square miles (2.9 km2) and colloquially known as \" \\\n",
    "           \"the Square Mile − retains boundaries that follow closely its medieval limits.\" \\\n",
    "           \"The City of Westminster is also an Inner London borough holding city status. \" \\\n",
    "           \"Greater London is governed by the Mayor of London and the London Assembly.\" \\\n",
    "           \"London is located in the southeast of England.\" \\\n",
    "           \"Westminster is located in London.\" \\\n",
    "           \"London is the biggest city in Britain. London has a population of 7,172,036.\"\n",
    "\n",
    "    sentences = getSentences(text)\n",
    "    nlp_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "    triples = []\n",
    "    print (text)\n",
    "    for sentence in sentences:\n",
    "        triples.append(processSentence(sentence))\n",
    "\n",
    "    printGraph(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtree_matcher(nlp(candidate_sentences['sentence'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = \"/Users/awenc/NUIM/CS440/KG_NLPSystem/data/sentences.csv\"\n",
    "candidate_sentences = pd.read_csv(path_to_csv)\n",
    "print(candidate_sentences.shape)\n",
    "print(candidate_sentences['sentence'].sample(5))\n",
    "print(candidate_sentences['sentence'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_sci_sm\")\n",
    "nlp =spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# nlpSci = spacy.load(\"en_core_sci_sm\")\n",
    "# nlpSci.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "# doc = nlpSci(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from app import clean\n",
    "for s in candidate_sentences['sentence']:\n",
    "    print(clean(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sent = candidate_sentences['sentence'][4]+\" watermelon such as melon\"\n",
    "get_relation(new_sent)\n",
    "# new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sentence):\n",
    "    # store entities in variable - object subject\n",
    "    ent_1 = ''\n",
    "    ent_2 = ''\n",
    "    tok_dep = '' # dependency tag of previous token in the sentence\n",
    "    tok_txt = '' # previous token in the senetence\n",
    "    pfx = ''\n",
    "    mod = ''\n",
    "\n",
    "    for tok in nlp(sentence):\n",
    "        if tok.dep_ != 'punct':\n",
    "            if tok.dep_ == 'compound':\n",
    "                pfx = tok.text\n",
    "                if tok.dep_ == 'compound':\n",
    "                    pfx = tok_txt +\" \"+tok.text\n",
    "\n",
    "        if tok.dep_.endswith('mod') == True:\n",
    "            mod = tok.text\n",
    "            if tok.dep_ == 'compound':\n",
    "                mod = tok_txt+\" \"+tok.text\n",
    "\n",
    "        if tok.dep_.find(\"subj\") == True:\n",
    "            ent_1 = mod+\" \"+pfx+\" \"+tok.text\n",
    "            tok_txt =''\n",
    "            tok_dep=''\n",
    "            pfx = ''\n",
    "            mod =''\n",
    "\n",
    "        if tok.dep_.find(\"obj\") == True:\n",
    "            ent_2 = mod+\" \"+pfx+\" \"+tok.text\n",
    "\n",
    "        tok_dep = tok.dep_\n",
    "        tok_txt = tok.text\n",
    "    return [ent_1.strip(), ent_2.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sent3(text)\n",
    "# get_sents(text)\n",
    "# graph(text)\n",
    "# sent_(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triple(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sent = []\n",
    "\n",
    "    for token in doc:\n",
    "        # if the token is a verb\n",
    "        if (token.pos_ in ['VERB','ROOT']):\n",
    "            phrase =''\n",
    "            # only extract noun or pronoun subjects\n",
    "            for sub_tok in token.lefts:\n",
    "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
    "                    # add subject to the phrase\n",
    "                    phrase += sub_tok.text\n",
    "                    # save the root of the verb in phrase\n",
    "                    phrase += ' '+token.lemma_\n",
    "                    # check for noun or pronoun direct objects\n",
    "                    for sub_tok in token.rights:\n",
    "                        # save the object in the phrase\n",
    "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
    "                            phrase += ' '+sub_tok.text\n",
    "                            sent.append(phrase)\n",
    "\n",
    "    return sent\n",
    "get_triple(fsent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "# \"[subject] ... initially founded\"\n",
    "pattern = [\n",
    "  # anchor token: founded\n",
    "  {\n",
    "    \"RIGHT_ID\": \"founded\",\n",
    "    \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n",
    "  },\n",
    "  # founded -> subject\n",
    "  {\n",
    "    \"LEFT_ID\": \"founded\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"subject\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}\n",
    "  },\n",
    "  # \"founded\" follows \"initially\"\n",
    "  {\n",
    "    \"LEFT_ID\": \"founded\",\n",
    "    \"REL_OP\": \";\",\n",
    "    \"RIGHT_ID\": \"initially\",\n",
    "    \"RIGHT_ATTRS\": {\"ORTH\": \"initially\"}\n",
    "  }\n",
    "]\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "matcher.add(\"FOUNDED\", [pattern])\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"anchor_founded\",\n",
    "        \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_subject\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"},\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"founded_object\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object_modifier\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n",
    "    }\n",
    "]\n",
    "pattern = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"anchor_founded\",       # unique name\n",
    "    \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}  # token pattern for \"founded\"\n",
    "  },\n",
    "    \n",
    "]\n",
    "matcher.add(\"FOUNDED\", [pattern])\n",
    "doc = nlp(\"Smith founded two companies.\")\n",
    "matches = matcher(doc)\n",
    "print(matches) # [(4851363122962674176, [1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "\n",
    "pattern = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"anchor_founded\",\n",
    "        \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_subject\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"},\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"founded_object\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object_modifier\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n",
    "    }\n",
    "]\n",
    "\n",
    "matcher.add(\"FOUNDED\", [pattern])\n",
    "doc = nlp(\"Lee, an experienced CEO, has founded two AI startups.\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(matches) # [(4851363122962674176, [6, 0, 10, 9])]\n",
    "# Each token_id corresponds to one pattern dict\n",
    "match_id, token_ids = matches[0]\n",
    "for i in range(len(token_ids)):\n",
    "    print(pattern[i][\"RIGHT_ID\"] + \":\", doc[token_ids[i]].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"MyCorp Inc. is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(doc.sents))\n",
    "\n",
    "# Examine the entities extracted by the mention detector.\n",
    "# Note that they don't have types like in SpaCy, and they\n",
    "# are more general (e.g including verbs) - these are any\n",
    "# spans which might be an entity in UMLS, a large\n",
    "# biomedical database.\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also visualise dependency parses\n",
    "# (This renders automatically inside a jupyter notebook!):\n",
    "from spacy import displacy\n",
    "displacy.render(next(doc.sents), style='dep', jupyter=True)\n",
    "\n",
    "# See below for the generated SVG.\n",
    "# Zoom your browser in a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,_ in enumerate(doc):\n",
    "\n",
    "    # doc[i].is_sent_start\n",
    "    print(doc[i].is_sent_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get root of sentence\n",
    "def n_chunk(sent):\n",
    "    roots = ''\n",
    "    doc = nlp(sent)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(f\"\\nChunk Text: {chunk.text}\\n-> Root: {chunk.root.text}\\n-> Arc label:{chunk.root.dep_}\\n-> Root head: {chunk.root.head.text}\\n\")\n",
    "        roots += chunk.root.text+ ' ' +chunk.root.head.text+' '\n",
    "\n",
    "    return roots\n",
    "\n",
    "n_chunk(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_chunks_sent(sentence):\n",
    "    '''\n",
    "    Take in single sntence and output\n",
    "    nound chunks\n",
    "    '''\n",
    "    chunks = []\n",
    "    sent = nlp(sentence)\n",
    "    for chunk in sent.noun_chunks:\n",
    "#         print(chunk.left())\n",
    "        chunks.append(chunk)\n",
    "#         print(\"Chunk: \",chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = noun_chunks_sent(txt2)\n",
    "len(chunks)\n",
    "chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnch():\n",
    "    doc = nlp(txt2)\n",
    "    # nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # doc = nlp(\"bright red apples on the tree\")\n",
    "    print(type(doc))\n",
    "#     print(len(doc)+\" Words added \")\n",
    "    for i, token in enumerate(doc):\n",
    "        \n",
    "        if doc[i].n_lefts != 0:\n",
    "            if doc[i].n_lefts > 1:\n",
    "                print(doc[i].n_lefts)\n",
    "                print([token.text for token in doc[i].lefts])\n",
    "        \n",
    "#         print([token.text for token in doc[i].lefts])\n",
    "#         print(type(doc[i].lefts))\n",
    "#         print([token.text for token in doc[i].lefts if token !=None])\n",
    "#         print([token.text for token in doc[i].lefts])  \n",
    "#         print([token.text for token in doc[i].rights])\n",
    "#         print(doc[i].n_lefts)  # 2\n",
    "#         print(doc[i].n_rights)  # 1\n",
    "#         # TODO Lookup\n",
    "#         doc[i].n_rights # give int value of how many something is to left or right?\n",
    "nnch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_component(text):\n",
    "    doc = nlp(text)\n",
    "    print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "    print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "    \n",
    "noun_component(txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: noun(subject), verb, noun(object)\n",
    "def get_triple(text):\n",
    "    doc = nlp(text)\n",
    "    sent = []\n",
    "\n",
    "    for token in doc:\n",
    "        # if the token is a verb\n",
    "        if (token.pos_ in ['VERB','ROOT']):\n",
    "            phrase =''\n",
    "            # only extract noun or pronoun subjects\n",
    "            for sub_tok in token.lefts:\n",
    "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
    "                    # add subject to the phrase\n",
    "                    phrase += sub_tok.text\n",
    "                    # save the root of the verb in phrase\n",
    "                    phrase += ' '+token.lemma_\n",
    "                    # check for noun or pronoun direct objects\n",
    "                    for sub_tok in token.rights:\n",
    "                        # save the object in the phrase\n",
    "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
    "                            phrase += ' '+sub_tok.text\n",
    "                            sent.append(phrase)\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_component_list(text):\n",
    "    doc = nlp(text)\n",
    "    chunks = list(doc.noun_chunks)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_component_list(txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For this i have installed python 3.6 and it do not work but, maybe I can make it to work later.\n",
    "# # Coreference resolution\n",
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "doc1 = nlp('My sister has a dog. She loves him.')\n",
    "print(doc1._.coref_clusters)\n",
    "\n",
    "doc2 = nlp('Angela lives in Boston. She is quite happy in that city.')\n",
    "for ent in doc2.ents:\n",
    "    print(ent._.coref_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get root of sentence\n",
    "def n_chunk(sent):\n",
    "    roots = ''\n",
    "    doc = nlp(sent)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(f\"\\nChunk Text: {chunk.text}\\n-> Root: {chunk.root.text}\\n-> Arc label:{chunk.root.dep_}\\n-> Root head: {chunk.root.head.text}\\n\")\n",
    "        roots += chunk.root.text+ ' ' +chunk.root.head.text+' '\n",
    "\n",
    "    return roots\n",
    "# print(n_chunk(get_sent(text)[1]))\n",
    "\n",
    "def noun_chunks_sent(sentence):\n",
    "    '''\n",
    "    Take in single sntence and output\n",
    "    nound chunks\n",
    "    '''\n",
    "\n",
    "    sent = nlp(sentence)\n",
    "    for chunk in sent.noun_chunks:\n",
    "        print(\"Chunk: \",chunk)\n",
    "\n",
    "def noun_component(text):\n",
    "    doc = nlp(text)\n",
    "    print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "    print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "def noun_component_list(text):\n",
    "    doc = nlp(text)\n",
    "    chunks = list(doc.noun_chunks)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_pairs = []\n",
    "relations = []\n",
    "nvn = []\n",
    "# relations = [get_relation(i) for i in tqdm(candidate_sentences['sentences'])]\n",
    "for i in candidate_sentences[\"sentences\"]:\n",
    "    try:\n",
    "        ent_pairs.append(ent_extraction(i))\n",
    "        relations.append(get_relation(i))\n",
    "        nvn.append(ent_extraction(i))\n",
    "        nvn.append(get_relation(i))\n",
    "    except TypeError as e:\n",
    "        print(i)\n",
    "        print(type(i))\n",
    "        print(e)\n",
    "        pass\n",
    "kg_df = pd.DataFrame({'source':object_, 'target':subject_, 'edge':relations})\n",
    "\n",
    "# candidate_sentences['sentences'][0]\n",
    "for i,j in zip(ent_pairs,relations):\n",
    "    print(i[0],j,i[1])\n",
    "# ent_pairs\n",
    "# relations\n",
    "# nvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileconvert():\n",
    "    \"\"\"\n",
    "    Read in direcory of files, look for .txt extension, extract sentences from text, save sentences in one file,\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    p = Path('/Users/awenc/NUIM/CS440/KG_NLPSystem/data/Psychology Test Materials')\n",
    "    for name in p.glob('*.txt'):\n",
    "        f = open(name, 'r')\n",
    "        line = f.read()\n",
    "        # print(line)\n",
    "        x = get_sent(line)\n",
    "        len(get_sent(line))\n",
    "        outfile = open(\"/Users/awenc/NUIM/CS440/KG_NLPSystem/workspace/sentences_psychology.txt\",'a')\n",
    "        type(x)\n",
    "        for i in clean(x):\n",
    "            # print(type(i)) # <class 'spacy.tokens.span.Span'>\n",
    "            # print(str(i))\n",
    "            outfile.write(str(i)+\"\\n\")\n",
    "    \"\"\"\n",
    "    Wrap each line in quotes in order to make data digastable by pandas DataFrame\n",
    "    \"\"\"\n",
    "    with open(\"/Users/awenc/NUIM/CS440/KG_NLPSystem/workspace/sentences_psychology.txt\",'r') as f:\n",
    "        x= f.readlines()\n",
    "        with open('/Users/awenc/NUIM/CS440/KG_NLPSystem/workspace/sentences_psychology.csv','w') as fw:\n",
    "            fw.write(\"sentences\"+\"\\n\")\n",
    "            for line in x:\n",
    "                fw.write('\\\"'+line.strip('\\n').strip('\\r')+'\\\"\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgsys2",
   "language": "python",
   "name": "kgsys2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}