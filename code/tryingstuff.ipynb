{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import spacy\n",
    "import scispacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "# import neuralcoref\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp =spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtree_matcher(doc):\n",
    "  subjpass = 0\n",
    "\n",
    "  for i,tok in enumerate(doc):\n",
    "    # find dependency tag that contains the text \"subjpass\"    \n",
    "    if tok.dep_.find(\"subjpass\") == True:\n",
    "      subjpass = 1\n",
    "\n",
    "  x = ''\n",
    "  y = ''\n",
    "\n",
    "  # if subjpass == 1 then sentence is passive\n",
    "  if subjpass == 1:\n",
    "    for i,tok in enumerate(doc):\n",
    "      if tok.dep_.find(\"subjpass\") == True:\n",
    "        y = tok.text\n",
    "\n",
    "      if tok.dep_.endswith(\"obj\") == True:\n",
    "        x = tok.text\n",
    "  \n",
    "  # if subjpass == 0 then sentence is not passive\n",
    "  else:\n",
    "    for i,tok in enumerate(doc):\n",
    "      if tok.dep_.endswith(\"subj\") == True:\n",
    "        x = tok.text\n",
    "\n",
    "      if tok.dep_.endswith(\"obj\") == True:\n",
    "        y = tok.text\n",
    "\n",
    "  return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags I've chosen for relations\n",
    "deps = [\"ROOT\", \"adj\", \"attr\", \"agent\", \"amod\"]\n",
    "\n",
    "# Tags I've chosen for entities(subjects and objects)\n",
    "deps = [\"compound\", \"prep\", \"conj\", \"mod\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sent_(text):\n",
    "    \"\"\"\n",
    "    Function prints out attributes of word from spacy.\n",
    "    Parameters\n",
    "    ----------\n",
    "        text : str\n",
    "            Block of text with multiple sentences\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    Prints attributes of token object.\n",
    "    text: Get the token text.\n",
    "    POS: part-of-speech tag.\n",
    "    DEP: dependency label.\n",
    "    \"\"\"\n",
    "    sent = nlp(text)\n",
    "    for token in sent:\n",
    "        # Get the token text, part-of-speech tag and dependency label\n",
    "        token_text = token.text\n",
    "        token_pos = token.pos_\n",
    "        token_dep = token.dep_\n",
    "        print('{:<12}{:<10}{:<10}{:<10}'.format(token_text, token_pos, token_dep,spacy.explain(token_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n",
    "def getSentences(text):\n",
    "    nlp = English()\n",
    "    nlp.add_pipe('sentencizer')\n",
    "    document = nlp(text)\n",
    "    return [sent.text for sent in document.sents]\n",
    "\n",
    "def printToken(token):\n",
    "    print(token.text, \"->\", token.dep_)\n",
    "\n",
    "def appendChunk(original, chunk):\n",
    "    return original + ' ' + chunk\n",
    "\n",
    "def isRelationCandidate(token):\n",
    "    deps = [\"ROOT\", \"adj\", \"attr\", \"agent\", \"amod\"]\n",
    "    return any(subs in token.dep_ for subs in deps)\n",
    "\n",
    "def isConstructionCandidate(token):\n",
    "    deps = [\"compound\", \"prep\", \"conj\", \"mod\"]\n",
    "    return any(subs in token.dep_ for subs in deps)\n",
    "\n",
    "def processSubjectObjectPairs(tokens):\n",
    "    subject = ''\n",
    "    object = ''\n",
    "    relation = ''\n",
    "    subjectConstruction = ''\n",
    "    objectConstruction = ''\n",
    "    for token in tokens:\n",
    "        printToken(token)\n",
    "        if \"punct\" in token.dep_:\n",
    "            continue\n",
    "        if isRelationCandidate(token):\n",
    "            relation = appendChunk(relation, token.lemma_)\n",
    "        if isConstructionCandidate(token):\n",
    "            if subjectConstruction:\n",
    "                subjectConstruction = appendChunk(subjectConstruction, token.text)\n",
    "            if objectConstruction:\n",
    "                objectConstruction = appendChunk(objectConstruction, token.text)\n",
    "        if \"subj\" in token.dep_:\n",
    "            subject = appendChunk(subject, token.text)\n",
    "            subject = appendChunk(subjectConstruction, subject)\n",
    "            subjectConstruction = ''\n",
    "        if \"obj\" in token.dep_:\n",
    "            object = appendChunk(object, token.text)\n",
    "            object = appendChunk(objectConstruction, object)\n",
    "            objectConstruction = ''\n",
    "\n",
    "    print (subject.strip(), \",\", relation.strip(), \",\", object.strip())\n",
    "    return (subject.strip(), relation.strip(), object.strip())\n",
    "\n",
    "def processSentence(sentence):\n",
    "    tokens = nlp_model(sentence)\n",
    "    return processSubjectObjectPairs(tokens)\n",
    "\n",
    "def printGraph(triples):\n",
    "    G = nx.Graph()\n",
    "    for triple in triples:\n",
    "        G.add_node(triple[0])\n",
    "        G.add_node(triple[1])\n",
    "        G.add_node(triple[2])\n",
    "        G.add_edge(triple[0], triple[1])\n",
    "        G.add_edge(triple[1], triple[2])\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "    plt.figure()\n",
    "    nx.draw(G, pos, edge_color='black', width=1, linewidths=1,\n",
    "            node_size=500, node_color='seagreen', alpha=0.9,\n",
    "            labels={node: node for node in G.nodes()})\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    text = \"London is the capital and largest city of England and the United Kingdom. Standing on the River \" \\\n",
    "           \"Thames in the south-east of England, at the head of its 50-mile (80 km) estuary leading to \" \\\n",
    "           \"the North Sea, London has been a major settlement for two millennia. \" \\\n",
    "           \"Londinium was founded by the Romans. The City of London, \" \\\n",
    "           \"London's ancient core − an area of just 1.12 square miles (2.9 km2) and colloquially known as \" \\\n",
    "           \"the Square Mile − retains boundaries that follow closely its medieval limits.\" \\\n",
    "           \"The City of Westminster is also an Inner London borough holding city status. \" \\\n",
    "           \"Greater London is governed by the Mayor of London and the London Assembly.\" \\\n",
    "           \"London is located in the southeast of England.\" \\\n",
    "           \"Westminster is located in London.\" \\\n",
    "           \"London is the biggest city in Britain. London has a population of 7,172,036.\"\n",
    "\n",
    "    sentences = getSentences(text)\n",
    "    nlp_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "    triples = []\n",
    "    print (text)\n",
    "    for sentence in sentences:\n",
    "        triples.append(processSentence(sentence))\n",
    "\n",
    "    printGraph(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtree_matcher(nlp(candidate_sentences['sentence'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = \"/Users/awenc/NUIM/CS440/KG_NLPSystem/data/sentences.csv\"\n",
    "candidate_sentences = pd.read_csv(path_to_csv)\n",
    "print(candidate_sentences.shape)\n",
    "print(candidate_sentences['sentence'].sample(5))\n",
    "print(candidate_sentences['sentence'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "nlp =spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlpSci = spacy.load(\"en_core_sci_sm\")\n",
    "# nlpSci.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "# doc = nlpSci(text)\n",
    "\n",
    "from data import *\n",
    "text = biosci01\n",
    "doc = nlp(text)\n",
    "sent01=candidate_sentences['sentence'][0]\n",
    "sent09=candidate_sentences['sentence'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent01\n",
    "sent09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matcher class object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#define the pattern\n",
    "pattern = [{'DEP':'amod', 'OP':\"?\"}, # adjectival modifier\n",
    "           {'POS':'NOUN'},\n",
    "           {'LOWER': 'such'},\n",
    "           {'LOWER': 'as'},\n",
    "           {'POS': 'PROPN'}]\n",
    "\n",
    "matcher.add(\"matching_1\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "span = doc[matches[0][1]:matches[0][2]]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from app import clean\n",
    "for s in candidate_sentences['sentence']:\n",
    "    print(clean(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sentence):\n",
    "    \"\"\"\n",
    "    Get relation within input sentenceence based on pattern provided.\n",
    "    params: str - input of single sentenceence.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Matcher class object\n",
    "    matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "    #define the pattern\n",
    "    # TODO: explore matcher\n",
    "\n",
    "    \"\"\"\n",
    "    Match 0 or more times / match 0 or 1 time(one relation in sencence?)\n",
    "    Dodałem PROPN ale jeszcze nie przetestowałem.\n",
    "    \"\"\"\n",
    "    # # define such as pattern\n",
    "    # suchaspattern = [{'POS':'NOUN'}, \n",
    "    #        {'LOWER': 'such'}, \n",
    "    #        {'LOWER': 'as'}, \n",
    "    #        {'POS': 'PROPN'} #proper noun]\n",
    "    # wrong pattern?\n",
    "    pattern = [{'DEP':'ROOT'},\n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},\n",
    "            {'POS':'PROPN','OP':'?'},\n",
    "            {'POS':'ADJ','OP':\"?\"}]\n",
    "    #define the pattern\n",
    "    suchas_pattern = [{'DEP':'amod', 'OP':\"?\"}, # adjectival modifier\n",
    "           {'POS':'NOUN'},\n",
    "           {'LOWER': 'such'},\n",
    "           {'LOWER': 'as'},\n",
    "           {'POS': 'PROPN'}]\n",
    "\n",
    "    andor_pattern = [{'DEP':'amod', 'OP':\"?\"}, \n",
    "           {'POS':'NOUN'}, \n",
    "           {'LOWER': 'and', 'OP':\"?\"}, \n",
    "           {'LOWER': 'or', 'OP':\"?\"}, \n",
    "           {'LOWER': 'other'}, \n",
    "           {'POS': 'NOUN'}] \n",
    "\n",
    "    es_pattern = [{'DEP':'nummod','OP':\"?\"}, \n",
    "           {'DEP':'amod','OP':\"?\"}, \n",
    "           {'POS':'NOUN'}, \n",
    "           {'IS_PUNCT':True}, \n",
    "           {'LOWER': 'especially'}, \n",
    "           {'DEP':'nummod','OP':\"?\"}, \n",
    "           {'DEP':'amod','OP':\"?\"}, \n",
    "           {'POS':'NOUN'}] \n",
    "    # TODO Add verb matcher? IDK\n",
    "\n",
    "    # matcher.add(\"matching_1\", None, pattern) \n",
    "\n",
    "    # TODO: obczaic kod.\n",
    "    # matches = matcher(doc) \n",
    "    \n",
    "    # print(span.text)\n",
    "    \n",
    "    # for matcher.add had to change for 2 arguments and second as list\n",
    "    # it happen after i had to downgrade python version to python=3.6\n",
    "    # now it gets 2 arguments instead of 3, latter is type list so we can\n",
    "    # add multiple patterns.\n",
    "    matcher.add(\"matching_1\", [pattern,suchas_pattern,andor_pattern,es_pattern])\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) - 1\n",
    "\n",
    "    # span = doc[matches[0][1]:matches[0][2]] \n",
    "    span = doc[matches[k][1]:matches[k][2]]\n",
    "\n",
    "    return(span.text)\n",
    "\n",
    "get_relation(candidate_sentences['sentence'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sent = candidate_sentences['sentence'][4]+\" watermelon such as melon\"\n",
    "get_relation(new_sent)\n",
    "# new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_sents(text)[0])\n",
    "# print(get_sents(text)[1])\n",
    "# print(get_sents(text)[2])\n",
    "# print('end of unclean\\n')\n",
    "\n",
    "# print(clean(get_sents(text)[0].strip()))\n",
    "# print(clean(get_sents(text)[1].strip()))\n",
    "# print(clean(get_sents(text)[2].strip()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sentence):\n",
    "    # store entities in variable - object subject\n",
    "    ent_1 = ''\n",
    "    ent_2 = ''\n",
    "    tok_dep = '' # dependency tag of previous token in the sentence\n",
    "    tok_txt = '' # previous token in the senetence\n",
    "    pfx = ''\n",
    "    mod = ''\n",
    "\n",
    "    for tok in nlp(sentence):\n",
    "        if tok.dep_ != 'punct':\n",
    "            if tok.dep_ == 'compound':\n",
    "                pfx = tok.text\n",
    "                if tok.dep_ == 'compound':\n",
    "                    pfx = tok_txt +\" \"+tok.text\n",
    "\n",
    "        if tok.dep_.endswith('mod') == True:\n",
    "            mod = tok.text\n",
    "            if tok.dep_ == 'compound':\n",
    "                mod = tok_txt+\" \"+tok.text\n",
    "\n",
    "        if tok.dep_.find(\"subj\") == True:\n",
    "            ent_1 = mod+\" \"+pfx+\" \"+tok.text\n",
    "            tok_txt =''\n",
    "            tok_dep=''\n",
    "            pfx = ''\n",
    "            mod =''\n",
    "\n",
    "        if tok.dep_.find(\"obj\") == True:\n",
    "            ent_2 = mod+\" \"+pfx+\" \"+tok.text\n",
    "\n",
    "        tok_dep = tok.dep_\n",
    "        tok_txt = tok.text\n",
    "    return [ent_1.strip(), ent_2.strip()]\n",
    "\n",
    "\n",
    "get_entities(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sent3(text)\n",
    "# get_sents(text)\n",
    "# graph(text)\n",
    "# sent_(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triple(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sent = []\n",
    "\n",
    "    for token in doc:\n",
    "        # if the token is a verb\n",
    "        if (token.pos_ in ['VERB','ROOT']):\n",
    "            phrase =''\n",
    "            # only extract noun or pronoun subjects\n",
    "            for sub_tok in token.lefts:\n",
    "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
    "                    # add subject to the phrase\n",
    "                    phrase += sub_tok.text\n",
    "                    # save the root of the verb in phrase\n",
    "                    phrase += ' '+token.lemma_\n",
    "                    # check for noun or pronoun direct objects\n",
    "                    for sub_tok in token.rights:\n",
    "                        # save the object in the phrase\n",
    "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
    "                            phrase += ' '+sub_tok.text\n",
    "                            sent.append(phrase)\n",
    "\n",
    "    return sent\n",
    "get_triple(fsent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(clean(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "# \"[subject] ... initially founded\"\n",
    "pattern = [\n",
    "  # anchor token: founded\n",
    "  {\n",
    "    \"RIGHT_ID\": \"founded\",\n",
    "    \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n",
    "  },\n",
    "  # founded -> subject\n",
    "  {\n",
    "    \"LEFT_ID\": \"founded\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"subject\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}\n",
    "  },\n",
    "  # \"founded\" follows \"initially\"\n",
    "  {\n",
    "    \"LEFT_ID\": \"founded\",\n",
    "    \"REL_OP\": \";\",\n",
    "    \"RIGHT_ID\": \"initially\",\n",
    "    \"RIGHT_ATTRS\": {\"ORTH\": \"initially\"}\n",
    "  }\n",
    "]\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "matcher.add(\"FOUNDED\", [pattern])\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"anchor_founded\",\n",
    "        \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_subject\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"},\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"founded_object\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object_modifier\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n",
    "    }\n",
    "]\n",
    "pattern = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"anchor_founded\",       # unique name\n",
    "    \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}  # token pattern for \"founded\"\n",
    "  },\n",
    "    \n",
    "]\n",
    "matcher.add(\"FOUNDED\", [pattern])\n",
    "doc = nlp(\"Smith founded two companies.\")\n",
    "matches = matcher(doc)\n",
    "print(matches) # [(4851363122962674176, [1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "\n",
    "pattern = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"anchor_founded\",\n",
    "        \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_subject\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"},\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"founded_object\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object_modifier\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n",
    "    }\n",
    "]\n",
    "\n",
    "matcher.add(\"FOUNDED\", [pattern])\n",
    "doc = nlp(\"Lee, an experienced CEO, has founded two AI startups.\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(matches) # [(4851363122962674176, [6, 0, 10, 9])]\n",
    "# Each token_id corresponds to one pattern dict\n",
    "match_id, token_ids = matches[0]\n",
    "for i in range(len(token_ids)):\n",
    "    print(pattern[i][\"RIGHT_ID\"] + \":\", doc[token_ids[i]].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"MyCorp Inc. is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(doc.sents))\n",
    "\n",
    "# Examine the entities extracted by the mention detector.\n",
    "# Note that they don't have types like in SpaCy, and they\n",
    "# are more general (e.g including verbs) - these are any\n",
    "# spans which might be an entity in UMLS, a large\n",
    "# biomedical database.\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also visualise dependency parses\n",
    "# (This renders automatically inside a jupyter notebook!):\n",
    "from spacy import displacy\n",
    "displacy.render(next(doc.sents), style='dep', jupyter=True)\n",
    "\n",
    "# See below for the generated SVG.\n",
    "# Zoom your browser in a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,_ in enumerate(doc):\n",
    "\n",
    "    # doc[i].is_sent_start\n",
    "    print(doc[i].is_sent_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # removing new line character\n",
    "    text = re.sub('\\n','', str(text))\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    # removing apostrophes\n",
    "    text = re.sub(\"'s\",'',str(text))\n",
    "    # removing hyphens\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"- \",'',str(text))\n",
    "    # removing quotation marks\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    # removing this �, guessing it was apostrophe\n",
    "    text = re.sub(\"�s\",'',str(text))\n",
    "    text = re.sub('[a-z]+�','', str(text))\n",
    "    text = re.sub('�[a-z]+','', str(text))\n",
    "    text = re.sub('(','',str(text))\n",
    "    text = re.sub(')','',str(text))\n",
    "    # removing paragraph numbers\n",
    "    text = re.sub('[0-9]+.\\t','',str(text))\n",
    "    # removing new line characters\n",
    "    text = re.sub('\\n ','',str(text))\n",
    "    text = re.sub('\\n',' ',str(text))\n",
    "    # removing apostrophes\n",
    "    text = re.sub(\"'s\",'',str(text))\n",
    "    # removing hyphens\n",
    "    text = re.sub(\"-\",' ',str(text))\n",
    "    text = re.sub(\"- \",'',str(text))\n",
    "    # removing quotation marks\n",
    "    text = re.sub('\\\"','',str(text))\n",
    "    # removing salutations\n",
    "    text = re.sub(\"Mr\\.\",'Mr',str(text))\n",
    "    text = re.sub(\"Mrs\\.\",'Mrs',str(text))\n",
    "    # removing any reference to outside text\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
    "    text = re.sub(' +',' ', str(text))\n",
    "\n",
    "    return text\n",
    "# type(clean(txt6))\n",
    "txt6 = clean(txt6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt034=\"The fox jumped over the log.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_(txt6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get root of sentence\n",
    "def n_chunk(sent):\n",
    "    roots = ''\n",
    "    doc = nlp(sent)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(f\"\\nChunk Text: {chunk.text}\\n-> Root: {chunk.root.text}\\n-> Arc label:{chunk.root.dep_}\\n-> Root head: {chunk.root.head.text}\\n\")\n",
    "        roots += chunk.root.text+ ' ' +chunk.root.head.text+' '\n",
    "\n",
    "    return roots\n",
    "\n",
    "n_chunk(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_chunks_sent(sentence):\n",
    "    '''\n",
    "    Take in single sntence and output\n",
    "    nound chunks\n",
    "    '''\n",
    "    chunks = []\n",
    "    sent = nlp(sentence)\n",
    "    for chunk in sent.noun_chunks:\n",
    "#         print(chunk.left())\n",
    "        chunks.append(chunk)\n",
    "#         print(\"Chunk: \",chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = noun_chunks_sent(txt2)\n",
    "len(chunks)\n",
    "chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnch():\n",
    "    doc = nlp(txt2)\n",
    "    # nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # doc = nlp(\"bright red apples on the tree\")\n",
    "    print(type(doc))\n",
    "#     print(len(doc)+\" Words added \")\n",
    "    for i, token in enumerate(doc):\n",
    "        \n",
    "        if doc[i].n_lefts != 0:\n",
    "            if doc[i].n_lefts > 1:\n",
    "                print(doc[i].n_lefts)\n",
    "                print([token.text for token in doc[i].lefts])\n",
    "        \n",
    "#         print([token.text for token in doc[i].lefts])\n",
    "#         print(type(doc[i].lefts))\n",
    "#         print([token.text for token in doc[i].lefts if token !=None])\n",
    "#         print([token.text for token in doc[i].lefts])  \n",
    "#         print([token.text for token in doc[i].rights])\n",
    "#         print(doc[i].n_lefts)  # 2\n",
    "#         print(doc[i].n_rights)  # 1\n",
    "#         # TODO Lookup\n",
    "#         doc[i].n_rights # give int value of how many something is to left or right?\n",
    "nnch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_component(text):\n",
    "    doc = nlp(text)\n",
    "    print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "    print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "    \n",
    "noun_component(txt2)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NA WARSZTAT\n",
    "\n",
    "def get_relation(sent):\n",
    "    \"\"\"\n",
    "    Get relation within input sentence based on pattern provided.\n",
    "    params: str - input of single sentence.\n",
    "    \"\"\"\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    # Matcher class object\n",
    "    matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "    #define the pattern\n",
    "    # TODO: explore matcher\n",
    "\n",
    "    \"\"\"\n",
    "    Match 0 or more times / match 0 or 1 time(one relation in sencence?)\n",
    "    Dodałem PROPN ale jeszcze nie przetestowałem.\n",
    "    \"\"\"\n",
    "    pattern = [{'DEP':'ROOT'},\n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},\n",
    "            {'POS':'PROPN','OP':'?'},\n",
    "            {'POS':'ADJ','OP':\"?\"}]\n",
    "    # TODO Add verb matcher?\n",
    "\n",
    "    matcher.add(\"matching_1\", None, pattern)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) - 1\n",
    "\n",
    "    span = doc[matches[k][1]:matches[k][2]]\n",
    "\n",
    "    return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### WARSZTAT\n",
    "def get_entities(sentence):\n",
    "    # store entities in variable - object subject\n",
    "    ent_1 = ''\n",
    "    ent_2 = ''\n",
    "    tok_dep = '' # dependency tag of previous token in the sentence\n",
    "    tok_txt = '' # previous token in the senetence\n",
    "    pfx = ''\n",
    "    mod = ''\n",
    "\n",
    "    for tok in nlp(sentence):\n",
    "        if tok.dep_ != 'punct':\n",
    "            if tok.dep_ == 'compound':\n",
    "                pfx = tok.text\n",
    "                if tok.dep_ == 'compound':\n",
    "                    pfx = tok_txt +\" \"+tok.text\n",
    "\n",
    "        if tok.dep_.endswith('mod') == True:\n",
    "            mod = tok.text\n",
    "            if tok.dep_ == 'compound':\n",
    "                mod = tok_txt+\" \"+tok.text\n",
    "\n",
    "        if tok.dep_.find(\"subj\") == True:\n",
    "            ent_1 = mod+\" \"+pfx+\" \"+tok.text\n",
    "            tok_txt =''\n",
    "            tok_dep=''\n",
    "            pfx = ''\n",
    "            mod =''\n",
    "\n",
    "        if tok.dep_.find(\"obj\") == True:\n",
    "            ent_2 = mod+\" \"+pfx+\" \"+tok.text\n",
    "\n",
    "        tok_dep = tok.dep_\n",
    "        tok_txt = tok.text\n",
    "    return [ent_1.strip(), ent_2.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: noun(subject), verb, noun(object)\n",
    "def get_triple(text):\n",
    "    doc = nlp(text)\n",
    "    sent = []\n",
    "\n",
    "    for token in doc:\n",
    "        # if the token is a verb\n",
    "        if (token.pos_ in ['VERB','ROOT']):\n",
    "            phrase =''\n",
    "            # only extract noun or pronoun subjects\n",
    "            for sub_tok in token.lefts:\n",
    "                if (sub_tok.dep_ in ['nsubj','nsubjpass']) and (sub_tok.pos_ in ['NOUN','PROPN','PRON']):\n",
    "                    # add subject to the phrase\n",
    "                    phrase += sub_tok.text\n",
    "                    # save the root of the verb in phrase\n",
    "                    phrase += ' '+token.lemma_\n",
    "                    # check for noun or pronoun direct objects\n",
    "                    for sub_tok in token.rights:\n",
    "                        # save the object in the phrase\n",
    "                        if (sub_tok.dep_ in ['dobj']) and (sub_tok.pos_ in ['NOUN','PROPN']):\n",
    "                            phrase += ' '+sub_tok.text\n",
    "                            sent.append(phrase)\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_component_list(text):\n",
    "    doc = nlp(text)\n",
    "    chunks = list(doc.noun_chunks)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_component_list(txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For this i have installed python 3.6 and it do not work but, maybe I can make it to work later.\n",
    "# # Coreference resolution\n",
    "# import spacy\n",
    "# import neuralcoref\n",
    "\n",
    "# nlp = spacy.load('en')\n",
    "# neuralcoref.add_to_pipe(nlp)\n",
    "# doc1 = nlp('My sister has a dog. She loves him.')\n",
    "# print(doc1._.coref_clusters)\n",
    "\n",
    "# doc2 = nlp('Angela lives in Boston. She is quite happy in that city.')\n",
    "# for ent in doc2.ents:\n",
    "#     print(ent._.coref_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get root of sentence\n",
    "def n_chunk(sent):\n",
    "    roots = ''\n",
    "    doc = nlp(sent)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(f\"\\nChunk Text: {chunk.text}\\n-> Root: {chunk.root.text}\\n-> Arc label:{chunk.root.dep_}\\n-> Root head: {chunk.root.head.text}\\n\")\n",
    "        roots += chunk.root.text+ ' ' +chunk.root.head.text+' '\n",
    "\n",
    "    return roots\n",
    "# print(n_chunk(get_sent(text)[1]))\n",
    "\n",
    "def noun_chunks_sent(sentence):\n",
    "    '''\n",
    "    Take in single sntence and output\n",
    "    nound chunks\n",
    "    '''\n",
    "\n",
    "    sent = nlp(sentence)\n",
    "    for chunk in sent.noun_chunks:\n",
    "        print(\"Chunk: \",chunk)\n",
    "\n",
    "def noun_component(text):\n",
    "    doc = nlp(text)\n",
    "    print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "    print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "def noun_component_list(text):\n",
    "    doc = nlp(text)\n",
    "    chunks = list(doc.noun_chunks)\n",
    "    return chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgsys2",
   "language": "python",
   "name": "kgsys2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}