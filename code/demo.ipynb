{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input -> <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from main import fileconvert, clean\n",
    "import pandas as pd\n",
    "from Triple import Triple\n",
    "from tqdm import tqdm\n",
    "from spacy.matcher import Matcher\n",
    "import spacy\n",
    "import scispacy\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep domain data separate.\n",
    "fileconvert(path_to_folder, path_to_csv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n",
      "886\n"
     ]
    }
   ],
   "source": [
    "# wiki_csv = \"/Users/awenc/NUIM/CS440/KG_NLPSystem/data/wiki_sentences_v2.csv\"\n",
    "data_csv = \"/Users/awenc/NUIM/CS440/KG_NLPSystem/data/sentences.csv\"\n",
    "psychology_csv = \"/Users/awenc/NUIM/CS440/KG_NLPSystem/data/sentences_psychology.csv\"\n",
    "\n",
    "# wiki_sentences = pd.read_csv(wiki_csv)\n",
    "data_sentences = pd.read_csv(data_csv)\n",
    "psychology_sentences = pd.read_csv(psychology_csv)\n",
    "\n",
    "# print(len(wiki_sentences['sentence']))\n",
    "print(len(data_sentences['sentences']))\n",
    "print(len(psychology_sentences['sentences']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans  \n",
    "def relation(sentence):\n",
    "    \"\"\"\n",
    "    Get relation within input sentenceence based on pattern provided.\n",
    "    params: str - input of single sentenceence.\n",
    "    \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Matcher class object\n",
    "    matcher = Matcher(nlp.vocab, validate=True)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Match 0 or more times / match 0 or 1 time(one relation in sencence?)\n",
    "    Dodałem PROPN ale jeszcze nie przetestowałem.\n",
    "    \"\"\"\n",
    "    pattern0=[{'POS': 'VERB', 'OP': '?'},\n",
    "            {'POS': 'ADV', 'OP': '*'},\n",
    "            {'OP': '*'}, # additional wildcard - match any text in between\n",
    "            {'POS': 'VERB', 'OP': '+'}]\n",
    "    pattern1 = [{'DEP':'ROOT'},\n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},\n",
    "            {'POS':'PROPN','OP':'?'},\n",
    "            {'POS':'ADJ','OP':\"?\"}]\n",
    "    pattern = [{'POS': 'VERB', 'OP': '?'},\n",
    "           {'POS': 'ADV', 'OP': '*'},\n",
    "           {'POS': 'AUX', 'OP': '*'},\n",
    "           {'POS': 'VERB', 'OP': '+'}]\n",
    "    pattern2 = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}] \n",
    "    # pattern = [{'POS': 'VERB', 'OP': '?'}, {'POS': 'ADV', 'OP': ''}, {'OP': ''}, {'POS': 'VERB', 'OP': '+'}]\n",
    "\n",
    "    matcher.add(\"Verb phrase\", [pattern2])\n",
    "\n",
    "    # call the matcher to find matches \n",
    "    matches = matcher(doc)\n",
    "    spans = [doc[start:end] for _, start, end in matches]\n",
    "\n",
    "    res = filter_spans(spans)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 886/886 [00:10<00:00, 87.64it/s]\n",
      "100%|██████████| 563/563 [00:00<00:00, 1397274.05it/s]\n"
     ]
    }
   ],
   "source": [
    "entity_pairs = []\n",
    "relations = []\n",
    "for i in tqdm(psychology_sentences['sentences']):\n",
    "    entity_pairs.append(entities(i))\n",
    "    relations.append(relation(i))\n",
    "\n",
    "data_entity_pairs = []\n",
    "data_relations = []\n",
    "for i in tqdm(data_sentences['sentences']):\n",
    "    data_entity_pairs.append(i)\n",
    "    data_relations.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 886/886 [00:05<00:00, 163.19it/s]\n",
      "100%|██████████| 886/886 [00:04<00:00, 189.26it/s]\n",
      "100%|██████████| 563/563 [00:02<00:00, 188.21it/s]\n",
      "100%|██████████| 563/563 [00:03<00:00, 177.91it/s]\n"
     ]
    }
   ],
   "source": [
    "relations2 = [relation(i) for i in tqdm(psychology_sentences['sentences'])]\n",
    "entities2 = [entities(i) for i in tqdm(psychology_sentences['sentences'])]\n",
    "\n",
    "data_entities = [entities(i) for i in tqdm(data_sentences['sentences'])]\n",
    "data_relations = [relation(i) for i in tqdm(data_sentences['sentences'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_twenty_two = \"A method for recovering structure and motion from a set of scaled orthographic silhouette views with unconstrained camer motion is presented. The outer epipolar tangencies between six or more views are used simultaneously to recover the relaticve pose of the cameras asn hance the cisual hull of the object. The camera represtation proposed permits a closed form solution in the optimazation of the camera parametrers. The resulting system is applied to the problem of reconstructing aircraft in flight.\"\n",
    "sent01 = \"Bobby was delighted he got a part in the school play, even though the part was a small one.\"\n",
    "sent02 = \"Because my porridge had gone cold, I heated it in the microwave.\"\n",
    "sent03 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because my porridge had gone cold, I heated it in the microwave.\n",
      "[heated]\n",
      "['I', 'microwave']\n",
      "[heated]\n"
     ]
    }
   ],
   "source": [
    "# doc = nlp(\"John and his son David went to the park after he found his keys. They spend 2 hours at the park and got icecream.\")\n",
    "doc = nlp(sent02)\n",
    "data_entity_pairs = []\n",
    "data_relations = []\n",
    "\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    data_entity_pairs.append(entities(str(sent)))\n",
    "    data_relations.append(relation(str(sent)))\n",
    "    print(relation(str(sent)))\n",
    "    \n",
    "    print(str(entities(str(sent))))\n",
    "    print(str(relation(str(sent))))\n",
    "# entities()\n",
    "object_ = [i[0] for i in data_entity_pairs]\n",
    "subject_ = [i[1] for i in data_entity_pairs]\n",
    "relations_ = [i for i in data_relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(data_entity_pairs))\n",
    "object_ = [i[0] for i in data_entities]\n",
    "subject_ = [i[1] for i in data_entities]\n",
    "relations_ = [i for i in data_relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Objects: 563 Relations: 1 Subjects: 563'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Objects: {len(object_)} Relations: {len(relations_)} Subjects: {len(subject_)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I heated microwave\n"
     ]
    }
   ],
   "source": [
    "from main import clean\n",
    "\n",
    "for i in range(len(relations_)):\n",
    "    output = str(object_[i]) + str(relations_[i]) + str(subject_[i])\n",
    "    # output = clean(output)\n",
    "    output = re.sub(\"\\[\",' ', str(output))\n",
    "    output = re.sub(\"\\]\",' ', str(output))\n",
    "    output = re.sub(' +',' ',str(output))\n",
    "    print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[heated]    1\ndtype: int64"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([str(i) for i in data_relations]).value_counts()[:50] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('kgsys2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4ef060659159fbeb4441d8e36b456b29bba2f0d8bf3938743d35f5235f44b89c"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}